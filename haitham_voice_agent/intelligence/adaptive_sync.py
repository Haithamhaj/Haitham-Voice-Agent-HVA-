import os
import logging
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from haitham_voice_agent.tools.memory.voice_tools import VoiceMemoryTools
from haitham_voice_agent.tools.memory.storage.sqlite_store import SQLiteStore
from haitham_voice_agent.config import Config

logger = logging.getLogger(__name__)

class AdaptiveSync:
    """
    Adaptive Sync & Learning System
    Detects offline file moves/renames and updates the Knowledge Base.
    """
    
    def __init__(self):
        self.memory_tools = VoiceMemoryTools()
        # We need direct access to SQLite for low-level index checks
        self.sqlite_store = SQLiteStore()
        
    async def ensure_initialized(self):
        await self.memory_tools.ensure_initialized()
        # SQLiteStore is initialized by memory_tools usually, but safe to init again
        await self.sqlite_store.initialize()

    def calculate_file_hash(self, file_path: Path) -> Optional[str]:
        """Calculate MD5 hash of a file (Digital Fingerprint)"""
        try:
            if not file_path.exists() or not file_path.is_file():
                return None
                
            # Limit to first 10MB to avoid freezing on huge files
            MAX_SIZE = 10 * 1024 * 1024 
            file_size = file_path.stat().st_size
            
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                if file_size > MAX_SIZE:
                    # Read head and tail for speed on large files
                    hasher.update(f.read(1024 * 1024)) # 1MB Head
                    f.seek(-1024 * 1024, 2)
                    hasher.update(f.read(1024 * 1024)) # 1MB Tail
                else:
                    # Read all
                    buf = f.read(65536)
                    while len(buf) > 0:
                        hasher.update(buf)
                        buf = f.read(65536)
                        
            return hasher.hexdigest()
        except Exception as e:
            logger.warning(f"Failed to hash {file_path}: {e}")
            return None

    async def sync_knowledge_base(self, scan_roots: List[str] = None) -> Dict[str, int]:
        """
        Scan file system and sync with DB.
        Detects:
        1. Moved files (Same Hash, Different Path) -> Update DB + Learn
        2. Renamed files (Same Hash, Different Name) -> Update DB + Learn
        3. New files -> Index
        """
        logger.info("ðŸ”„ Starting Adaptive Sync (Offline Learning)...")
        await self.ensure_initialized()
        
        if not scan_roots:
            scan_roots = [
                str(Path.home() / "Documents"),
                str(Path.home() / "Applications"),
                str(Path.home() / "Software")
            ]
            
        stats = {
            "scanned": 0,
            "learned_moves": 0,
            "new_indexed": 0,
            "errors": 0
        }
        
        for root_str in scan_roots:
            root = Path(root_str)
            if not root.exists():
                continue
                
            for current_path in root.rglob("*"):
                if current_path.is_file() and not current_path.name.startswith("."):
                    try:
                        stats["scanned"] += 1
                        
                        # 1. Calculate Hash
                        file_hash = self.calculate_file_hash(current_path)
                        if not file_hash:
                            continue
                            
                        # 2. Check DB for this Hash
                        # We need a method to find by hash. Let's add it or use raw query.
                        # For now, we'll use a raw query helper here or assume we add it to SQLiteStore.
                        # Let's use raw query via sqlite_store connection for now to avoid modifying store yet.
                        
                        existing_record = await self._find_file_by_hash(file_hash)
                        
                        if existing_record:
                            old_path = Path(existing_record["path"])
                            
                            # 3. Detect Change
                            if old_path.resolve() != current_path.resolve():
                                # MOVED or RENAMED!
                                logger.info(f"ðŸŽ“ LEARNING: Detected move {old_path.name} -> {current_path.name}")
                                
                                # Update DB
                                await self.memory_tools.memory_system.index_file(
                                    path=str(current_path),
                                    project_id=existing_record["project_id"], # Keep project or re-evaluate? Keep for now.
                                    description=existing_record["description"],
                                    tags=json.loads(existing_record["tags"]) if existing_record["tags"] else [],
                                    file_hash=file_hash
                                )
                                
                                # TODO: Delete old record? Or mark as moved?
                                # For now, index_file with same hash might duplicate if path is PK.
                                # SQLiteStore schema: path is PRIMARY KEY.
                                # So we have a new record. We should delete the old one if it doesn't exist anymore.
                                if not old_path.exists():
                                    # It was indeed moved.
                                    # We can't delete easily without a delete method in MemorySystem, 
                                    # but we can leave it or implement delete.
                                    pass
                                    
                                stats["learned_moves"] += 1
                        else:
                            # New File
                            # Index it? Maybe too expensive to index everything on startup.
                            # Let's only index if it looks important or user requested "Full Sync".
                            # For "Adaptive Learning", we care mostly about MOVES of known files.
                            pass
                            
                    except Exception as e:
                        logger.error(f"Error syncing {current_path}: {e}")
                        stats["errors"] += 1
                        
        logger.info(f"âœ… Adaptive Sync Complete: {stats}")
        return stats

    async def _find_file_by_hash(self, file_hash: str) -> Optional[Dict[str, Any]]:
        """Helper to find file by hash"""
        try:
            import aiosqlite
            import json
            async with aiosqlite.connect(self.sqlite_store.db_path) as db:
                db.row_factory = aiosqlite.Row
                async with db.execute("SELECT * FROM file_index WHERE file_hash = ?", (file_hash,)) as cursor:
                    row = await cursor.fetchone()
                    if row:
                        return dict(row)
            return None
        except Exception:
            return None
