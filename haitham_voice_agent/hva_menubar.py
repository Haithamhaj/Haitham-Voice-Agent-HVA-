"""
HVA Menu Bar App
Simple menu bar application with global keyboard shortcut
"""

import rumps
import threading
import sys
import os
import asyncio
import multiprocessing
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from haitham_voice_agent.gui_process import run_gui_process
from haitham_voice_agent.wake_word import get_detector
from haitham_voice_agent.tools.voice.stt import STTHandler
from haitham_voice_agent import llm_router
from haitham_voice_agent.dispatcher import dispatch_action
from haitham_voice_agent.tts import get_tts
from haitham_voice_agent.tools.advisor import get_advisor
from haitham_voice_agent.tools.system_awareness import get_system_awareness
from haitham_voice_agent.tools.notifications.manager import NotificationManager
from haitham_voice_agent.tools.system_tools import SystemTools
import time



class HVAMenuBarApp(rumps.App):
    def __init__(self):
        super(HVAMenuBarApp, self).__init__(
            "ğŸ¤ HVA",
            icon=None,
            quit_button=None
        )
        
        # Start GUI Process
        self.gui_queue = multiprocessing.Queue()
        self.cmd_queue = multiprocessing.Queue()
        self.gui_process = multiprocessing.Process(target=run_gui_process, args=(self.gui_queue, self.cmd_queue))
        self.gui_process.daemon = True
        self.gui_process.start()
        
        # Setup TTS Callback
        self.tts = get_tts()
        self.tts.set_callback(self._on_tts_speak)
        
        # Initialize STT
        self.stt = STTHandler()
        
        self.detector = get_detector()
        self.is_listening = False
        self.listen_thread = None
        
        # Start command listener thread
        self.cmd_thread = threading.Thread(target=self._listen_for_gui_commands)
        self.cmd_thread.daemon = True
        self.cmd_thread.start()
        
        # Start Advisor Background Thread
        self.advisor_thread = threading.Thread(target=self._run_advisor_checks)
        self.advisor_thread.daemon = True
        self.advisor_thread.start()
        
        # Start Ollama Warm-up Thread (to load model into VRAM)
        self.warmup_thread = threading.Thread(target=self._warmup_ollama)
        self.warmup_thread.daemon = True
        self.warmup_thread.start()
        
        # Initialize System Awareness
        self.system_awareness = get_system_awareness()
        self.system_awareness.start()
        
        # Start Notification Manager Thread
        self.notification_thread = threading.Thread(target=self._run_notifications)
        self.notification_thread.daemon = True
        self.notification_thread.start()
        
        # Menu items
        self.menu = [
            rumps.MenuItem("ğŸ¤ Listen (âŒ˜â‡§H)", callback=self.start_listening),
            rumps.separator,
            rumps.MenuItem("ğŸ“ Show Window", callback=self.show_window),
            rumps.MenuItem("ğŸ”„ Reset State", callback=self.reset_state),
            rumps.MenuItem("ğŸ—‘ï¸ Clear History", callback=self.clear_history),
            rumps.separator,
            rumps.MenuItem("â„¹ï¸  About", callback=self.show_about),
            rumps.separator,
            rumps.MenuItem("â¹ï¸  Quit", callback=self.quit_app),
        ]

    def _run_advisor_checks(self):
        """Run periodic advisor checks (Resources & Wellbeing)"""
        advisor = get_advisor()
        while True:
            try:
                # Check Resources
                res_alert = advisor.check_resources()
                if res_alert:
                    self.gui_queue.put(('add_message', 'error', res_alert, True))
                    
                # Check Wellbeing
                health_alert = advisor.check_wellbeing()
                if health_alert:
                    self.gui_queue.put(('add_message', 'info', health_alert, True))
                    
                # Sleep 5 minutes
                time.sleep(300)
            except Exception as e:
                print(f"Advisor check error: {e}")
                time.sleep(60)

    def _warmup_ollama(self):
        """Warm up Ollama model in background"""
        try:
            # Removed sleep to improve responsiveness
            
            print("ğŸ”¥ Warming up Ollama...")
            from haitham_voice_agent.ollama_orchestrator import get_orchestrator
            orchestrator = get_orchestrator()
            
            # Simple hello to force model load
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(orchestrator.classify_request("hello"))
                print("âœ… Ollama Warmed Up & Ready!")
            finally:
                loop.close()
        except Exception as e:
            print(f"âš ï¸ Ollama Warmup Failed: {e}")

    def _run_notifications(self):
        """Run notification manager in background loop"""
        try:
            print("ğŸ”” Starting Notification Manager...")
            system_tools = SystemTools()
            manager = NotificationManager(system_tools)
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(manager.start())
            finally:
                loop.close()
        except Exception as e:
            print(f"âš ï¸ Notification Manager Failed: {e}")

    def _on_tts_speak(self, text):
        """Callback when TTS speaks (or would speak)"""
        # Forward to GUI as assistant message
        # We use auto_close=False so user can read it
        self.gui_queue.put(('add_message', 'assistant', text, False))
        
    def _listen_for_gui_commands(self):
        """Listen for commands sent from the GUI (Manual Input)"""
        while True:
            try:
                cmd_type, content = self.cmd_queue.get()
                if cmd_type == 'command':
                    print(f"âŒ¨ï¸  Manual Command: {content}")
                    # Process the command directly
                    threading.Thread(target=self._process_text_command, args=(content,)).start()
                elif cmd_type == 'listen':
                    print("ğŸ¤ GUI requested listening")
                    # Start listening in main thread context (or just call start_listening)
                    # We use rumps.timer or just call it directly since start_listening handles threading
                    self.start_listening(None)
            except Exception as e:
                print(f"Error in command listener: {e}")
                
    def _process_text_command(self, text):
        """Process a text command (same logic as voice but skips STT)"""
        try:
            # Detect wake word (optional for manual input, but good for consistency)
            has_wake_word, command = self.detector.detect(text)
            if not has_wake_word:
                command = text
                
            print(f"âš™ï¸  Processing Manual: {command}")
            self.gui_queue.put(('set_agent_status', 'ollama', 'Analyzing Intent...'))
            
            # --- 1. Ollama Orchestrator (Local Intelligence) ---
            from haitham_voice_agent.ollama_orchestrator import get_orchestrator
            orchestrator = get_orchestrator()
            
            # Run async classification in sync context
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                classification = loop.run_until_complete(orchestrator.classify_request(command))
            finally:
                loop.close()
                
            plan = None
            
            if classification.get("type") == "direct_response":
                print("Ollama handled request directly.")
                self.gui_queue.put(('set_agent_status', 'ollama', 'Generating Response...'))
                self.gui_queue.put(('add_message', 'assistant', classification["response"], True))
                # Speak it too
                self.tts.speak(classification["response"])
                return
                
            elif classification.get("type") == "execute_command":
                print(f"Ollama identified command: {classification['intent']}")
                self.gui_queue.put(('set_agent_status', 'tool', f"Action: {classification['intent']}"))
                plan = {
                    "intent": classification["intent"],
                    "steps": [{
                        "tool": "system", 
                        "action": classification["intent"],
                        "params": classification.get("parameters", {})
                    }],
                    "confirmation_needed": False
                }
            
            else:
                # Delegate to GPT
                print(f"Ollama delegated to: {classification.get('delegate_to')}")
                self.gui_queue.put(('set_agent_status', 'gpt', 'Consulting Cloud Brain...'))
                
                # Get router instance
                router = llm_router.get_router()
                
                # Generate execution plan
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    plan = loop.run_until_complete(router.generate_execution_plan(command))
                finally:
                    loop.close()
            
            if not plan:
                self.gui_queue.put(('add_message', 'error', 'Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø£Ù…Ø± / Could not understand command', True))
                self.gui_queue.put(('set_agent_status', 'idle', 'Ready'))
                return
            
            # Show plan intent
            intent = plan.get('intent', 'unknown')
            self.gui_queue.put(('add_message', 'info', f"Intent: {intent}", False))
            self.gui_queue.put(('set_agent_status', 'tool', f"Executing: {intent}"))
            
            # Execute action
            result = dispatch_action(plan)
            
            # Show result
            if result.get('success'):
                response = result.get('message', 'ØªÙ… Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­')
                self.gui_queue.put(('add_message', 'assistant', response, True))
                
                # Show additional info if available
                if 'data' in result:
                    data_str = str(result['data'])
                    # Check if it looks like a file list (contains newlines and filenames)
                    if '\n' in data_str and ('.txt' in data_str or '.pdf' in data_str or '.py' in data_str):
                         self.gui_queue.put(('add_message', 'file_list', data_str, True))
                    else:
                         self.gui_queue.put(('add_message', 'success', data_str[:200], True))
            else:
                error_msg = result.get('message', 'Ø­Ø¯Ø« Ø®Ø·Ø£')
                self.gui_queue.put(('add_message', 'error', error_msg, True))
            
            # Notification
            rumps.notification(
                title="âœ… HVA Done",
                subtitle="",
                message="ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ù…Ø± / Command executed"
            )
            
        except Exception as e:
            print(f"âŒ Error processing manual command: {e}")
            import traceback
            traceback.print_exc()
            self.gui_queue.put(('add_message', 'error', f'Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}', True))
            self.gui_queue.put(('set_agent_status', 'idle', 'Error'))

    def start_listening(self, _):
        """Start listening for voice command"""
        print(f"ğŸ¤ start_listening called. is_listening={self.is_listening}")
        
        if self.is_listening:
            # Check if thread is actually alive
            if self.listen_thread and self.listen_thread.is_alive():
                print("âš ï¸ Already listening and thread is alive. Ignoring.")
                return
            else:
                print("âš ï¸ is_listening was True but thread is dead. Resetting.")
                self.is_listening = False
        
        self.is_listening = True
        
        # Run in separate thread to not block UI
        self.listen_thread = threading.Thread(target=self._listen_and_process)
        self.listen_thread.daemon = True
        self.listen_thread.start()

    def reset_state(self, _):
        """Force reset application state"""
        self.is_listening = False
        self.listen_thread = None
        self.gui_queue.put(('set_agent_status', 'idle', 'State Reset'))
        rumps.notification("HVA", "", "State reset / ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø©")
    
    def _listen_and_process(self):
        """Listen for voice and process command"""
        try:
            # Show listening indicator (Pulse)
            self.gui_queue.put(('show_listening',))
            
            # Listen for voice
            print("ğŸ¤ Listening...")
            text = self.stt.listen_realtime()
            
            if not text:
                # Just a timeout, not a critical error
                self.gui_queue.put(('add_message', 'info', "Listening paused (Timeout) / Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙˆÙ‚Øª", True))
                self.is_listening = False
                self.gui_queue.put(('set_agent_status', 'idle', 'Ready'))
                return
            
            print(f"ğŸ“ Transcribed: {text}")
            
            # Detect wake word
            has_wake_word, command = self.detector.detect(text)
            
            if has_wake_word:
                self.gui_queue.put(('add_message', 'user', command, False))
            else:
                # No wake word, use full text as command
                self.gui_queue.put(('add_message', 'user', text, False))
                command = text
            
            # Show processing
            self.gui_queue.put(('show_processing',))
            self.gui_queue.put(('set_agent_status', 'ollama', 'Analyzing Voice...'))
            
            # Process command
            print(f"âš™ï¸  Processing: {command}")
            
            try:
                # --- 1. Ollama Orchestrator (Local Intelligence) ---
                from haitham_voice_agent.ollama_orchestrator import get_orchestrator
                orchestrator = get_orchestrator()
                
                # Run async classification in sync context
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    classification = loop.run_until_complete(orchestrator.classify_request(command))
                finally:
                    loop.close()
                    
                plan = None
                
                if classification.get("type") == "direct_response":
                    print("Ollama handled request directly.")
                    self.gui_queue.put(('set_agent_status', 'ollama', 'Generating Response...'))
                    self.gui_queue.put(('add_message', 'assistant', classification["response"], True))
                    # Speak it too (fire and forget task)
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(self.tts.speak(classification["response"]))
                    finally:
                        loop.close()
                    
                    self.is_listening = False # Reset listening state
                    return
                    
                elif classification.get("type") == "execute_command":
                    print(f"Ollama identified command: {classification['intent']}")
                    self.gui_queue.put(('set_agent_status', 'tool', f"Action: {classification['intent']}"))
                    plan = {
                        "intent": classification["intent"],
                        "steps": [{
                            "tool": "system", 
                            "action": classification["intent"],
                            "params": classification.get("parameters", {})
                        }],
                        "confirmation_needed": False
                    }
                
                else:
                    # Delegate to GPT
                    print(f"Ollama delegated to: {classification.get('delegate_to')}")
                    self.gui_queue.put(('set_agent_status', 'gpt', 'Consulting Cloud Brain...'))
                    
                    # Get router instance
                    router = llm_router.get_router()
                    
                    # Generate execution plan (run async in sync context)
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        plan = loop.run_until_complete(router.generate_execution_plan(command))
                    finally:
                        loop.close()
                
                if not plan:
                    self.gui_queue.put(('add_message', 'error', 'Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø£Ù…Ø± / Could not understand command', True))
                    self.is_listening = False
                    self.gui_queue.put(('set_agent_status', 'idle', 'Ready'))
                    return
                
                # Show plan intent
                intent = plan.get('intent', 'unknown')
                self.gui_queue.put(('add_message', 'info', f"Intent: {intent}", False))
                self.gui_queue.put(('set_agent_status', 'tool', f"Executing: {intent}"))
                
                # Execute action
                result = dispatch_action(plan)
                
                # Show result
                if result.get('success'):
                    response = result.get('message', 'ØªÙ… Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­')
                    self.gui_queue.put(('add_message', 'assistant', response, True))
                    
                    # Show additional info if available
                    if 'data' in result:
                        data_str = str(result['data'])
                        # Check if it looks like a file list
                        if '\n' in data_str and ('.txt' in data_str or '.pdf' in data_str or '.py' in data_str):
                             self.gui_queue.put(('add_message', 'file_list', data_str, True))
                        elif "Organized" in data_str or "Cleaned" in data_str or "Briefing" in data_str:
                             # Show full report for organizer/briefing
                             self.gui_queue.put(('add_message', 'success', data_str, True))
                        else:
                             self.gui_queue.put(('add_message', 'success', data_str[:500], True))
                else:
                    error_msg = result.get('message', 'Ø­Ø¯Ø« Ø®Ø·Ø£')
                    self.gui_queue.put(('add_message', 'error', error_msg, True))
                
                # Notification
                rumps.notification(
                    title="âœ… HVA Done",
                    subtitle="",
                    message="ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ù…Ø± / Command executed"
                )
                
            except Exception as e:
                print(f"âŒ Error processing command: {e}")
                self.gui_queue.put(('add_message', 'error', f'Ø®Ø·Ø£: {str(e)}', True))
                self.gui_queue.put(('set_agent_status', 'idle', 'Error'))
                
        except Exception as e:
            print(f"âŒ Error in listen_and_process: {e}")
            self.gui_queue.put(('add_message', 'error', f'Ø®Ø·Ø£: {str(e)}', True))
        
        finally:
            self.is_listening = False
    
    def show_window(self, _):
        """Show the GUI window"""
        self.gui_queue.put(('show',))
    
    def clear_history(self, _):
        """Clear the window history"""
        self.gui_queue.put(('clear',))
        rumps.notification(
            title="ğŸ—‘ï¸  HVA",
            subtitle="",
            message="ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ø³Ø¬Ù„ / History cleared"
        )
    
    def show_about(self, _):
        """Show about dialog"""
        rumps.alert(
            title="ğŸ¤ Haitham Voice Agent",
            message=(
                "ÙˆÙƒÙŠÙ„Ùƒ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø°ÙƒÙŠ\n"
                "Your Smart Voice Assistant\n\n"
                "âŒ¨ï¸  Ø§Ø¶ØºØ· âŒ˜â‡§H Ù„Ù„Ø§Ø³ØªÙ…Ø§Ø¹\n"
                "ğŸ¤ Ù‚Ù„ 'Ù‡ÙŠØ«Ù…' + Ø£Ù…Ø±Ùƒ\n"
                "ğŸ“ Ø´Ø§Ù‡Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ù†Ø§ÙØ°Ø©\n\n"
                "Made with â¤ï¸ by Haitham"
            )
        )
    
    def quit_app(self, _):
        """Quit the application"""
        if self.gui_process:
            self.gui_process.terminate()
        rumps.quit_application()


def main():
    """Main entry point"""
    # Fix for multiprocessing on macOS
    multiprocessing.set_start_method('spawn', force=True)
    
    app = HVAMenuBarApp()
    
    # Register global hotkey (Cmd+Shift+H)
    try:
        from pynput import keyboard
        
        def on_activate():
            print("ğŸ¹ Hotkey pressed!")
            app.start_listening(None)
        
        # Define hotkey combination
        hotkey = keyboard.HotKey(
            keyboard.HotKey.parse('<cmd>+<shift>+h'),
            on_activate
        )
        
        def for_canonical(f):
            return lambda k: f(keyboard_listener.canonical(k))
        
        # Start keyboard listener in background
        keyboard_listener = keyboard.Listener(
            on_press=for_canonical(hotkey.press),
            on_release=for_canonical(hotkey.release)
        )
        keyboard_listener.start()
        
        print("âœ… Global hotkey registered: Cmd+Shift+H")
        
    except Exception as e:
        print(f"âš ï¸  Could not register global hotkey: {e}")
        print("You can still use the menu bar to activate listening.")
    
    # Show welcome notification
    rumps.notification(
        title="ğŸ¤ HVA Started",
        subtitle="",
        message="Ø§Ø¶ØºØ· âŒ˜â‡§H Ù„Ù„Ø¨Ø¯Ø¡ / Press âŒ˜â‡§H to start"
    )
    
    # Run the app
    app.run()


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        import traceback
        error_msg = f"CRITICAL STARTUP ERROR: {e}\n{traceback.format_exc()}"
        print(error_msg)
        with open("/tmp/hva_startup_crash.log", "w") as f:
            f.write(error_msg)
        raise
